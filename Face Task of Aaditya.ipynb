{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.applications import VGG16\n",
    "\n",
    "\n",
    "# VGG16 was designed to work on 224 x 224 pixel input images sizes\n",
    "img_rows = 224\n",
    "img_cols = 224 \n",
    "      \n",
    "#Loads the VGG\n",
    "model = VGG16(weights = 'imagenet', \n",
    "                 include_top = False, \n",
    "                 input_shape = (img_rows, img_cols, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a Face recognizer with VGG16\n",
    "\n",
    "### Loading the VGG16 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inpsecting each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 InputLayer False\n",
      "1 Conv2D True\n",
      "2 Conv2D True\n",
      "3 MaxPooling2D True\n",
      "4 Conv2D True\n",
      "5 Conv2D True\n",
      "6 MaxPooling2D True\n",
      "7 Conv2D True\n",
      "8 Conv2D True\n",
      "9 Conv2D True\n",
      "10 MaxPooling2D True\n",
      "11 Conv2D True\n",
      "12 Conv2D True\n",
      "13 Conv2D True\n",
      "14 MaxPooling2D True\n",
      "15 Conv2D True\n",
      "16 Conv2D True\n",
      "17 Conv2D True\n",
      "18 MaxPooling2D True\n"
     ]
    }
   ],
   "source": [
    "# Let's print our layers \n",
    "for (i,layer) in enumerate(model.layers):\n",
    "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's freeze all layers except the top 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 InputLayer False\n",
      "1 Conv2D False\n",
      "2 Conv2D False\n",
      "3 MaxPooling2D False\n",
      "4 Conv2D False\n",
      "5 Conv2D False\n",
      "6 MaxPooling2D False\n",
      "7 Conv2D False\n",
      "8 Conv2D False\n",
      "9 Conv2D False\n",
      "10 MaxPooling2D False\n",
      "11 Conv2D False\n",
      "12 Conv2D False\n",
      "13 Conv2D False\n",
      "14 MaxPooling2D False\n",
      "15 Conv2D False\n",
      "16 Conv2D False\n",
      "17 Conv2D False\n",
      "18 MaxPooling2D False\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG16\n",
    "\n",
    "# VGG16 was designed to work on 224 x 224 pixel input images sizes\n",
    "img_rows = 224\n",
    "img_cols = 224 \n",
    "\n",
    "# Re-loads the VGG16 model without the top or FC layers\n",
    "model = VGG16(weights = 'imagenet', \n",
    "                 include_top = False, \n",
    "                 input_shape = (img_rows, img_cols, 3))\n",
    "\n",
    "# Here we freeze the last 4 layers \n",
    "# Layers are set to trainable as True by default\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False #your said images are not of \n",
    "    \n",
    "# Let's print our layers \n",
    "for (i,layer) in enumerate(model.layers):\n",
    "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make a function that returns our FC Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addTopModel(bottom_model, num_classes, D=256):\n",
    "    \"\"\"creates the top or head of the model that will be \n",
    "    placed ontop of the bottom layers\"\"\"\n",
    "    top_model = bottom_model.output\n",
    "    top_model = Flatten(name = \"flatten\")(top_model)\n",
    "    top_model = Dense(D, activation = \"relu\")(top_model)\n",
    "    #top_model = Dropout(0.3)(top_model)\n",
    "    top_model = Dense(num_classes, activation = \"softmax\")(top_model)\n",
    "    return top_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_2:0' shape=(None, 224, 224, 3) dtype=float32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x1e34e737eb8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e34e737f28>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e34e751208>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x1e34e7516d8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e34e751320>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e34e751f98>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x1e34e757908>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e34e7575c0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e34e75f2e8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e34e75fcc0>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x1e34e76a5f8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e34e76a400>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e34e76afd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e34e76d9e8>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x1e34e775320>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e34e775128>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e34e775cf8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e34e77f710>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x1e34e77ffd0>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's add our FC Head back onto VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               6422784   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 21,137,986\n",
      "Trainable params: 6,423,298\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "FC_Head = addTopModel(model, num_classes)\n",
    "\n",
    "modelnew = Model(inputs=model.input, outputs=FC_Head)\n",
    "\n",
    "print(modelnew.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 68 images belonging to 2 classes.\n",
      "Found 11 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    " \n",
    "train_data_dir = 'C:/Users/DELL/Desktop/TASK4/Facerecognitions/train'\n",
    "validation_data_dir = 'C:/Users/DELL/Desktop/TASK4/Facerecognitions/val'\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=20,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      horizontal_flip=True, \n",
    "      fill_mode='nearest')\n",
    " \n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    " \n",
    "# Change the batchsize according to your system RAM\n",
    "train_batchsize = 16\n",
    "val_batchsize = 10\n",
    " \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=train_batchsize,\n",
    "        class_mode='categorical')\n",
    " \n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=val_batchsize,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "30/30 [==============================] - 124s 4s/step - loss: 0.8064 - accuracy: 0.8137 - val_loss: 0.1423 - val_accuracy: 0.7273\n",
      "Epoch 2/3\n",
      "30/30 [==============================] - 121s 4s/step - loss: 0.1360 - accuracy: 0.9387 - val_loss: 0.0015 - val_accuracy: 0.7273\n",
      "Epoch 3/3\n",
      "30/30 [==============================] - 124s 4s/step - loss: 0.0457 - accuracy: 0.9853 - val_loss: 0.0028 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "                   \n",
    "modelnew.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = Adam(lr=0.001),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "nb_train_samples = 1190\n",
    "\n",
    "nb_validation_samples = 170\n",
    "epochs = 3\n",
    "batch_size = 16\n",
    "\n",
    "history = modelnew.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = 30,\n",
    "    epochs = epochs,\n",
    "    verbose=1,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = 8)\n",
    "\n",
    "modelnew.save(\"just.h5\")#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelnew.save('just.h5')\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.layers import Flatten\n",
    "#model.Flatten()\n",
    "classifier = modelnew\n",
    "classifier.load_weights('just.h5')\n",
    "#path_list = [\"C:/Users/DELL/Desktop/TASK4/Facerecognitions/val/dwayne_johnson/i1.jpg\",\"C:/Users/DELL/Desktop/TASK4/Facerecognition/val/keanu_reeves/b1.jpg\",\"C:/Users/DELL/Desktop/TASK4/10_Personality/train/ben_afflek/c1.jpg\",\"C:/Users/DELL/Desktop/TASK4/10_Personality/val/simon_pegg/f1.jpg\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "test_image_1 = image.load_img(\"C:/Users/DELL/Desktop/TASK4/Facerecognitions/train/will_smith/a1.jpg\",target_size=(224,224))\n",
    "path = \"C:/Users/DELL/Desktop/TASK4/Facerecognitions/val/will_smith/a1.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 224, 224, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image = image.img_to_array(test_image_1)\n",
    "test_image = np.expand_dims(test_image_1, axis=0)\n",
    "test_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "result = classifier.predict(test_image)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_img(name,path_f):\n",
    "    import cv2\n",
    "    \n",
    "    imgshow = cv2.imread(path_f) \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "\n",
    "    # org\n",
    "    org = (25, 25) \n",
    "\n",
    "    # fontScale \n",
    "    fontScale = 0.5\n",
    "\n",
    "    # Blue color in BGR \n",
    "    color = (255, 0, 0) \n",
    "\n",
    "    # Line thickness of 2 px \n",
    "    thickness = 1\n",
    "\n",
    "    # Using cv2.putText() method \n",
    "    image_f = cv2.putText(imgshow, name, org, font,  \n",
    "                       fontScale, color, thickness, cv2.LINE_AA) \n",
    "\n",
    "    # Displaying the image \n",
    "    cv2.imshow('test', image_f)  \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_m(result,path):\n",
    "    imgs = '0'\n",
    "    \n",
    "    if result[0][0] >= 1:\n",
    "        imgs = 'ben_affle'\n",
    "        show_img(imgs ,path) \n",
    "    elif result[0][1] >= 1:\n",
    "        imgs = 'Will_smith'\n",
    "        show_img(imgs ,path) \n",
    "    else:\n",
    "        imgs = \"I dont know, cool\"\n",
    "        show_img(imgs,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pred_m(result,path)\n",
    "    "
   ]
  },


